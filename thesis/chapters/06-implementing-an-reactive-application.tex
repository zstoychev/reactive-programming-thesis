\chapter{Реализация на реактивно приложение}

В края нека да разгледаме едно цялостно приложение, изградено чрез реактивни принципи. Ще разгледаме не голямо примерно приложение, чиято цел е да покаже различни особености на реактивните системи.

Приложението е разделено на две напълно независими функционалности. Първата от тях ни позволява да видим как да имплементираме силно интензивни за процесорите изчислителни операции, поддържайки автоматично разпределение на задачи и мониторинг на натовареността, с цел осигуряване на еластичност.

Другата функционалност разглежда персистентността при реактивните системи и комбинирането ѝ с архитектурните подходи, които разгледахме в предишната глава, както и получаване на данни от тях в реално време.

\section{Изчислителни възли и мониторинг за еластичност}

Първо ще разгледаме функционалността, натоварваща изчислителните ресурси, която ще бъде част от изчислителните възли. Ще разгледаме приложение, при което всеки потребител може да въведе дългоизпълняващ се JavaScript код. Този код бива изпратен до сървъра, който го изпълнява и изчислява краен резултат. Допълнително, всеки такъв код има възможност да публикува междинни резултати чрез извикване на \code{context.notify} метод от неговия контекст. Тези междинни резултати ще бъдат изпращани обратно на клиентите в реално време.

Пълният код може да се види в \shortlabeledref{приложение}{att:reactive-application}.

\subsection{Работници}

Нека дефинираме обща схема за управление на работа от изчислителните възли. Работата ще бъде извършвана от специални актьори, наречени работници. На всеки от изчислителните ресурси, за всеки тип изчислителни задачи, ще имаме главен актьор за задачата, \code{Master}, и съответни му няколко работника \code{Worker}, които той управлява. Когато очакваме задачи, които натоварват изчислителните ресурси, то броят на тези работници трябва да се избере спрямо броя на ядрата на системата.
 
От главния актьор ще искаме няколко неща. Първо той трябва да управлява задачите, да ги разпределя по работниците и да следи за тяхното изпълнение. Второ, ще искаме да балансираме максимално работата между работниците, като се предпазим от опасността да се запълни паметта на системата, ако задачите за изпълнение идват с по-бързо темпо от изпълнението им. За целта няма да препращаме всяка пристигнала задача веднага на работниците, ами ще ги съхраняваме в \emph{явна ограничена опашка}. При пристигане на съобщения при препълнена опашка веднага ще изпращаме съобщения за грешка на изпращача.

Можем да дефинираме следния протокол за комуникация на главния актьор с останалите участници:

\begin{lstlisting}
case class Work(work: Any)
case class WorkData(work: Any, sender: ActorRef)
case class ProcessData(data: WorkData, startTime: Long)
case class DataProcessed(data: WorkData, startTime: Long, result: Any)
case class ProcessFailed(data: WorkData, startTime: Long, reason: Throwable)
case object WorkerReady
\end{lstlisting}

За връщане на изчисления резултат на изпращачите ще използваме \code{Try} съобщения. Имплементацията на работниците е кратка:

\begin{lstlisting}
trait Worker extends Actor {
  import Master._
  import context.dispatcher
  
  def doWork(processData: ProcessData): Future[Any]
  
  context.parent ! WorkerReady
  
  def receive: Receive = {
    case p@ProcessData(data, startTime) =>
      doWork(p)
        .map { DataProcessed(data, startTime, _) }
        .recover { case NonFatal(e) => ProcessFailed(data, startTime, e) }
        .pipeTo(context.parent)
  }
}
\end{lstlisting}

По интересен е главният актьор:

\begin{lstlisting}
class Master(val statsMonitor: ActorRef)
            (worker: Props, val numberOfWorkers: Int)
            extends Actor with ProcessTimeTicker {
  import Master._
  
  var pendingWorkers = Queue.empty[ActorRef]
  var pendingWork = Queue.empty[WorkData]
  
  val workers = List.fill(numberOfWorkers)(context.actorOf(worker))
  
  def processWork(): Unit = ???
  def addToPending(worker: ActorRef) = ????
  
  def masterBehaviour: Receive = {
    case Work(work) =>
      logNewRequest()
      if (pendingWork.size >= MaxQueueSize)
        sender() ! Failure(new Exception)
      else {
        pendingWork = pendingWork enqueue WorkData(work, sender())
        processWork()
      }
    case DataProcessed(data, startTime, result) =>
      logProcessTime(startTime)
      addToPending(sender())
      data.sender ! Success(result)
    case ProcessFailed(data, startTime, reason) =>
      logProcessTime(startTime)
      addToPending(sender())
      data.sender ! Failure(reason)
    case WorkerReady => addToPending(sender())
  }
  
  def receive: Receive = masterBehaviour orElse processTimeTick
}
\end{lstlisting}

Той създава и управлява неговите работници. Подразбиращата се схема на \code{supervising} му позволява работниците да се рестартират автоматично, когато някой от тях даде грешка. Допълнителното нещо, което виждаме за главния актьор. е следенето на статистика за неговата комуникация чрез методи на \code{ProcessTimeTicker}. Повече за това ще видим в по-следващата секция.

Остава да имплементираме конкретен работник, който да изпълнява JavaScript кода. За целта ще използваме имплемантацията \code{Nashorn}, предоставена от Java 8, която е и подходящо защитена среда. Към всеки код, който ще бъде изпълнен, ще добавим и методи, чрез които той може да уведоми клиента с междинен резултат от изчислението (\code{notify}), както и да приспи текущата нишка за известно време (\code{sleep}). Кодът на работника може да се види в \shortlabeledref{приложение}{att:reactive-application}. Реализацията използва отделна среда за изпълнени на операциите за да не се натовари средата на останалите актьори в системата.

Така можем да стартираме тази система на изчислителните възли:

\begin{lstlisting}
val codeProcessingMaster = system.actorOf(
  Master.props(codeProcessingMonitor)
  (Props[ScalaCodeRunningWorker], Runtime.getRuntime.availableProcessors()),
  "codeProcessor"
)
\end{lstlisting}

\subsection{Връзка с клиента}

Възлите с REST интерфейс ще предоставят на клиента връзка с нашата система. Ще наричаме тези възли още \englishterm{frontend} възли. Те ще балансират натоварването към гглавните актьори на всеки от изчислителните възли. Akka предоставя различни маршрутизатори, които извършват това по определени стратегии. В нашата имплементация ще използваме маршрутизатор, който чрез постоянни съобщения следи натовареността на системата и свободната при нея памет. Неговото предимство е, че той маршрутизира задачи към най-малко натоварения възел, като освен това не изпраща задачи към силно натоварени системи. Така той осигурява устойчивост чрез успокояване на потока към услугите (подобно на прекъсвача на верига). Ще конфигурираме маршрутизатор в конфигурационния файл на \englishterm{frontend} приложенията и ще създадем инстанция за всеки от тях:

\begin{lstlisting}
akka.actor.deployment {
  /codeProcessingRouter = {
    router = cluster-metrics-adaptive-group
    metrics-selector = mix
    routees.paths = ["/user/codeProcessor"]
    cluster {
      enabled = on
      use-role = calculation
      allow-local-routees = off
    }
  }
}
\end{lstlisting}

Ще използваме комуникация по уеб сокети. Рамката Play, която стои зад \englishterm{frontend} възлите, предоставя възможност за управлението им чрез актьори, което ще направим тук. Отдолу цялата входно/изходна система на Play минава през асинхронна \englishterm{iteratee} имплементация на потоци.

\begin{lstlisting}
class CodeProcessorActor(codeProcessingRouter: ActorRef, out: ActorRef) extends Actor {
  def receive: Receive = {
    case code: String =>
    codeProcessingRouter ! Work(code)
    context.become(processing)
  }
    
  def processing: Receive = {
    case StatusUpdate(status) => out ! status
    case Success(result) =>
      out ! ("" + result) // using quotes for avoiding null
      context.stop(self)
    case Failure(e) =>
      out ! e.toString
      context.stop(self)
  }
}
\end{lstlisting}

\begin{lstlisting}
class CodeProcessor @Inject() (@Named("codeProcessingRouter") codeProcessingRouter: ActorRef) extends Controller {
  def processCode = WebSocket.acceptWithActor[String, String] {
    request => out =>
    Props(new CodeProcessorActor(codeProcessingRouter, out))
  }
}
\end{lstlisting}

Актьорът директно препраща всяко от съобщенията си към потребителя.

\subsection{Мониторинг за еластичност}

Важен аспект на реактивните системи е тяхната еластичност. За целта е необходим мониторинг на тяхната производителност, който да определя дали са необходими по-малко или повече възли (от някой тип). В нашата архитектура добавянето и извеждането на възли е безпроблемно.

Мониторингът трябва да комуникира със специален компонент, чиято цел е да управлява инстанциите на възлите. Изключително подходящи за това са различни облачни услуги, които предоставят лесен достъп до машини. Тази част зависи от конкретния случай, поради което няма да я разгледаме тук, а единствено ще реализираме мониторинг.

За мониторинг може да се подходи по различен начин. Единият от тях е следене на процесорите и паметта на системите, чрез инструментите, които видяхме в предишната секция. Това би било подходящо за повечето възли, но при изчислителните възли процесорът следва да бъде натоварен през повечето време. При тях като алтернатива можем да следим по колко заявки в секунда пристигат и средно по колко се забавят заявките и да приложим закона на Литъл за да изчислим колко паралелни инстанции ни трябват. Кодът за това може да бъде видян в \shortlabeledref{приложение}{att:reactive-application}.

\section{Персистентност}

В предишната глава представихме един различен модел на данните. Нека да обобщим какво ще искаме да постигнем чрез него:

\begin{itemize*}
  \item разделяне на компонентите, обработващи команди, от тези, обработващи заявки и съхраняване на състоянието при комадната част чрез събитиен журнал от промени;
  
  \item командните компоненти ще реализираме чрез актьори, които енкапсулират в себе си състоянието на цял един агрегат \cite{evans2003DDD}. Консистентността на състоянието на актьора ще гарантира консистентността на агрегата, но не и на връзките му с външния свят;
  
  \item заявките ще бъдат обработвани от други актьори, наричани изгледи, които също енкапсулират своят изглед на данните, получени от един или няколко агрегата;
  
  \item ще искаме да имаме една единствена инстанция за даден агрегат/изглед в цялото приложение, но инстанциите на различните агрегати/изгледи да са разпределени измежду различните възли в клъстера;
  
  \item трябва да можем да осъществим връзка с агрегат/изглед само по подаден идентификатор, независимо от неговото местоположение;
  
  \item при отказ на актьора или възела, при частично деление на мрежата или при друг проблем, ще искаме той да може да се стартира на друго местоположение, използвайки данните, записани в базата от данни;
  
  \item ще искаме при неактивност агрегата да се самоизключва за да не хаби памет и ресурси. При поискване трябва да може да се зареди от състоянието си в базата.
  
  \item Докато е активен всички данни се кешират в актьора, като се осигурява и техния запис в базата. Така данните са налични ефективно през актьора, но и съхранени сигурно.
\end{itemize*}

Оказва се, че Akka имплементира различни шаблони, чрез които могат да се постигнат именно тези свойства. Всички те са възможни именно благодарение на съобщенията и тяхната асинхронност и целеустремеността към реактивните принципи.

Ще разгледаме още как можем поточно да предаваме данни от изгледите. В тази част ще участват \englishterm{backend} и \englishterm{frontend} възлите.

В примерното приложение се разглежда имплементация на сайт, предоставящ възможност за създаване на прости анкети. При създаване се въвеждат възможните опции, след като всички с достъп до генериран линк могат въвеждат отговори, да променят стари и да трият. Отговорите са тип „Да/Не“. Всички промени се предават поточно по уеб сокети. Анкетата върви редом с чат за дискусии, който предоставя на потребителите последните 100 съобщения.

\subsection{Модел и команди}

Всяка една анкета има следната структура:

\begin{lstlisting}
case class PollAnswer(id: Int, name: String, optionsAnswers: List[Boolean])
case class Poll(id: String, options: List[String],
  answers: List[PollAnswer])
\end{lstlisting}

Към нея трябва да генерираме възможни команди, на всяка от които трябва да съответства по едно събитие. Към командите ще генерираме и съобщения за потвърждаване на успешно извършена операция, които най-много ще предоставят новосъздаден идентификатор.

\begin{lstlisting}
sealed trait PollCommand
case class StartPoll(options: List[String]) extends PollCommand
case class AnswerPoll(name: String, optionsAnswers: List[Boolean]) extends PollCommand
case class UpdatePollAnswer(id: Int, optionsAnswers: List[Boolean]) extends PollCommand
case class RemovePollAnswer(id: Int) extends PollCommand

sealed trait PollEvent
case class PollStarted(id: String, options: List[String]) extends PollEvent
case class PollAnswered(id: Int, name: String,
                        optionsAnswers: List[Boolean]) extends PollEvent
case class PollAnswerUpdated(id: Int, optionsAnswers: List[Boolean]) extends PollEvent
case class PollAnswerRemoved(id: Int) extends PollEvent

case class StartPollAck(id: String)
case class AnswerPollAck(id: Int)
case object UpdatePollAnswerAck
case object RemovePollAnswerAck
\end{lstlisting}

Подобни обекти ще направим и за чат агрегата.

Както споменахме, ще използваме актьор за да моделираме една анкета. Различните актьори ще разпределим по клъстера, използвайки механизма, който разгледохме в \labeledref{секция}{sec:cluster-sharding}. Актьорите ще разположим на \englishterm{backend} възлите, а останалите типове възли могат да ги достъпват чрез специално прокси. Всеки \englishterm{backend} възел стартира така конфигуриран актьор, през който ще се осъществява достъпа:

\begin{lstlisting}
val polls = ClusterSharding(system).start(
  typeName = PollActor.shardName,
  entityProps = Props(new ShardedEntityWithBackoff((id, passivator) =>
    Props(new PollActor(id, passivator, pollsViews))
  )),
  settings = ClusterShardingSettings(system),
  extractEntityId = PollActor.extractEntityId,
  extractShardId = PollActor.extractShardId
)
\end{lstlisting}

Такива типове актьори трябва да бъдат създадени за всеки от възможните типове агрегати и изгледи. \code{extractEntityId} определя как от съобщение, пратено на актьора \code{polls}, се извежда идентификатора на актьора, за да може то да бъде доставено и евентуално актьорът създаден. Ще използваме двойки (идентификатор, съобщение).

Нашите актьори са персистентни, което значи, че всички зависят от базата и биха отказали едновременно, ако се получи неѝн отказ при например натоварване. Ако след това всички веднага се опитат да достъпят базата отново, то това единствено би я натоварило още повече. Затова имплементираме обхващащ актьор \code{ShardedEntityWithBackoff}, който добавя специален \englishterm{supervisor}. При грешки той не рестартира веднага актьора, а опитва да го направи на постепенно увеличаващи се интервали, отместени с определен случаен фактор, за да се намали вероятността няколко актьора едновременно да достъпят базата. Така получаваме устойчивост, следвайки принципите, за които говорихме и при шаблона прекъсвач на верига.

Един персистентен актьор изглежда по следния начин:

\begin{lstlisting}[style=listing, caption={Персистентен актьор за анкета}]
class PollActor(id: String, passivator: ActorRef,
                pollViews: ActorRef) extends PersistentActor {
  import PollState._
  import Protocol._
  val SnapshotTarget = 20
  
  def persistenceId: String = s"poll-$id"
  
  var state: Option[PollState] = None
  
  context.setReceiveTimeout(2.minutes)
  
  def behaviour(state: Receive) =
    state orElse passivate orElse invalidCommand
  def complete(event: PollEvent, reply: Any,
               newState: => Receive) = persist(event) { event =>
    state = updateState(state, event)
    if (lastSequenceNr % SnapshotTarget == 0) saveSnapshot(state)
    context.become(newState)
    pollViews ! (id, Update(await = true))
  }
  
  def receiveCommand: Receive = behaviour(notStarted)
  
  def notStarted: Receive = {
    case StartPoll(options) =>
      complete(PollStarted(id, options),
               StartPollAck(id),
               behaviour(started(state.get)))
  }
  
  def started(ps: PollState): Receive = {
    case AnswerPoll(name, optionsAnswers)
      if ps.poll.options.size != optionsAnswers.size =>
      sender() ! InvalidState
    case AnswerPoll(name, optionsAnswers) =>
      val answerId = ps.totalAnswers + 1
      complete(PollAnswered(answerId, name, optionsAnswers),
               AnswerPollAck(answerId),
               behaviour(started(state.get)))
    
    case UpdatePollAnswer(id, optionsAnswers)
      if ps.poll.options.size != optionsAnswers.size || !ps.poll.answers.exists(_.id == id) =>
      sender() ! InvalidState
    case UpdatePollAnswer(id, optionsAnswers) =>
      complete(PollAnswerUpdated(id, optionsAnswers),
               UpdatePollAnswerAck,
               behaviour(started(state.get)))
    
    case RemovePollAnswer(id) if !ps.poll.answers.exists(_.id == id) =>
      sender() ! InvalidState
    case RemovePollAnswer(id) =>
      complete(PollAnswerRemoved(id),
               RemovePollAnswerAck,
               behaviour(started(state.get)))
  }
  
  def passivate: Receive = {
    case ReceiveTimeout => passivator ! Passivate(stopMessage = Stop)
    case Stop =>
    context.stop(self)
    context.parent ! PoisonPill
  }
  
  def invalidCommand: Receive = {
    case _: PollChatCommand => sender() ! InvalidState
  }
  
  def receiveRecover: Receive = {
    case event: PollEvent => state = updateState(state, event)
    case SnapshotOffer(_, snapshot) =>
      state = snapshot.asInstanceOf[Option[PollState]]
    case RecoveryCompleted => context.become(state match {
      case Some(ps) => behaviour(started(ps))
      case None => behaviour(notStarted)
    })
  }
}
\end{lstlisting}

Всеки актьор предоставя \code{persistenceId}, което определя неговия идентификатор в базата. Съобщенията в актьора се разделят на два пътя — \code{receiveCommand} и \code{receiveRecover}. Първото поведение се използва при нормална работата на актьора, когато той получава команди. Второто поведение е за когато той се възстановява от базата. Тогава той получава събитията едно по едно, според което трябва да обнови своето състояние. Ако е налична снимка на състоянието, то актьорът първо се възстановява от нея (ред 71), след което биват предадени последващите събития.

При нормална работа актьорът приема команди и запазва съответните им събития едно по едно. По подразбиране следващата команда не може да пристигне, преди предишна да е била записана. След записа се обновява състоянието и се уведомяват изгледите чрез \code{Update} съобщение (редове от 15 до 21). Освен това актьорът прави валидация на съобщенията и отхвърля тези, които биха довели до невалидно състояние.

На всеки две минути на неактивност, което се конфигурира на ред 11, актьорът изпраща специално съобщение за това, че желае да спре с цел да не хаби ресурси (\code{Passivate}, последвано от \code{Stop}).

Можем да забележим, че актьорът може да бъде стартиран дори и за несъществуващи агрегати, тоест за несъществуващ идентификатор. Тогава обаче, ако той не получи команда за създаване (ред 26), той автоматично ще отговаря със съобщение за невалидно състояние на всяко друго съобщение и след 2 минути неактивност ще се унищожи без никакви следи в базата.

\subsection{Заявки и изгледи}

Да разгледаме съответен изглед, чието състояние е почти едно към едно:

\begin{lstlisting}[style=listing, caption={Изглед на анкета}]
class PollViewActor(id: String, passivator: ActorRef) extends PersistentView {
  import PollViewActor._
  
  def persistenceId: String = s"poll-$id"
  
  def viewId: String = s"poll-$id-view"
  
  var state = Option.empty[PollState]
  
  var watchers = Set.empty[ActorRef]
  
  context.setReceiveTimeout(2.minutes)
  
  def receive: Receive = passivate orElse {
    case event: PollEvent => state = updateState(state, event)
    case SnapshotOffer(_, stateSnapshot) =>
      state = stateSnapshot.asInstanceOf[Option[PollState]]
    case QueryPoll(streaming) => state match {
      case Some(pollState) =>
        sender() ! pollState.poll
      if (streaming) {
        watchers += sender()
        context.watch(sender())
        context.setReceiveTimeout(Duration.Undefined)
      }
      case None => sender() ! InvalidState
    }
    case Terminated(watcher) =>
      watchers -= watcher
      if (watchers.isEmpty) context.setReceiveTimeout(2.minutes)
  }
  
  def passivate: Receive = {
    case ReceiveTimeout => passivator ! Passivate(stopMessage = Stop)
    case Stop =>
      context.stop(self)
      context.parent ! PoisonPill
  }
}
\end{lstlisting}

Изгледите, предоставени в текущата версия на Akka, са силно ограничени по това, че могат да наблюдават събитията само на един агрегат. При стандартен \code{event sourcing} те трябва да могат да се регистрират за всякакви събития. По-късно в секцията ще имплементираме именно такъв агрегиращ изглед.

Изгледите в Akka четат събитията, записани в базата, чрез периодично извличане на определен интервал. Освен това могат да бъдат накарани да ги извлекат веднага чрез \code{Update} съобщение. Тяхното \code{receive} поведение получава новите събития, както и заявки за данните. Тук актьорът позволява на клиентите да се регистрират за поток от промени. Тези регистрации обаче ще бъдат изгубени, ако той бъде рестартиран. Затова ако има регистрирани клиент той изключва автоматичното си спиране поради период на неактивност. Той все още може да бъде изключен поради повреда. Поради това наблюдаващите клиенти трябва също да регистрират \code{context.watch} наблюдение над изгледа, за да знаят в какво състояние са и да се пререгистрират при нужда.

За да изградим изгледи върху няколко агрегата е необходимо единичните изгледи да препращат събитията към интересуващи се актьори. Така например може да се реализира \englishterm{singleton} (\shortlabeledref{секция}{sec:singleton-actors-elasticity}) изглед, броящ всички чат съобщения в системата:

\begin{lstlisting}
class TotalChatMessagesView extends PersistentActor {
  import TotalChatMessagesView._
  
  def persistenceId: String = "total-chat-messages-view"
  
  var totalMessages = 0
  
  def receiveCommand: Receive = {
    case cm: ChatMessagePosted => persist(cm) { _ =>
      totalMessages += 1
    }
    case GetTotalMessages => sender() ! TotalMessages(totalMessages)
  }
      
  def receiveRecover: Receive = {
    case cm: ChatMessagePosted => totalMessages += 1
  }
}
\end{lstlisting}

Изгледът на конкретен чат препраща всички съобщения и към този изглед. За по-голяма консистентност е необходимо да се имплементира система, която гарантират, че съобщенията ще бъдат получени или препратени по-късно.

Изгледът на чат съобщения от своя страна притежава по-различно състояние — той запазва единствено последните 100 съобщения и я добър пример за ползите на това разделение. Може да бъде видян в \shortlabeledref{приложение}{att:reactive-application}.

\subsection{\englishterm{Frontend} код}

\englishterm{Frontend} възлите автоматично комуникират през прокси с агрегатите на \englishterm{backend} възлите. Командите минават през стандартни REST пътища:

\begin{lstlisting}
class Polls @Inject() (@Named("polls") polls: ActorRef,
                       @Named("pollsChats") pollsChats: ActorRef,
                       @Named("pollsViews") pollsViews: ActorRef,
                       @Named("pollsChatViews") pollsChatViews: ActorRef)
                       extends Controller {
  implicit val timeout = Timeout(4.seconds)
  
  def handlePollAck(ack: Future[Any]) = {
    ack
    .map {
      case InvalidState => Conflict
      case StartPollAck(id) => Ok(JsString(id))
      case AnswerPollAck(id) => Ok(JsNumber(id))
      case UpdatePollAnswerAck => Ok
      case RemovePollAnswerAck => Ok
    }
    .recover { case _: AskTimeoutException => ServiceUnavailable }
  }
  
  def startPoll = Action.async(parse.json[StartPoll]) { request =>
    val id = UUID.randomUUID.toString
    
    Future.sequence(List(
      polls ? (id, request.body),
      pollsChats ? (id, InitializePollChat)
    )) flatMap { case List(pollAck, _) =>
      handlePollAck(Future.successful(pollAck))
    }
  }
  
  def answerPoll(id: String) =  Action.async(parse.json[AnswerPoll]) { request =>
    handlePollAck(polls ? (id, request.body))
  }
...
}
\end{lstlisting}

Листингът показва част от кода на \englishterm{frontend} контролера, като на всеки REST път е съпоставено определено действие. Асинхронните действия в Play връщат функционални \englishterm{future} обекти. Тук можем да видим как тези действия, както и командите, които описахме по-рано, не са само CRUD глаголи, а вече носят семантиката за действията — стартиране (а не създаване) на анкета, попълване/отговаряне на анкета (а не създаване на запис за попълнени данни). Чрез тези подходи се доближаваме до Domain-Driven Design \cite{evans2003DDD}.

Друга много важна особеност тук е \emph{времеограничението, което слагаме на заявките}. Тук очакваме те да бъдат отговаряни бързо и ако това не се получи генерираме грешка за \code{ServiceUnavailable}.

Контролерът дефинира и два потока — на промени по анкетите и на чат съобщения, реализирани чрез актьори, които могат да бъдат разгледани в приложението.

\section{Клиентски код}

Реализацията включва и примерен клиентски код, при който сме използвали AngularJS и уеб сокети за разпространение на \englishterm{dataflow} потока на приложението.

Много важно при всички реактивни клиентски приложения е дизайнът на потребителския интерфейс да приема характеристиките на реактивните системи и да уведомява потребителите за всички възникнали действия — особено включително грешки, неактивност на някои услуги, понякога за евентуална консистентност и други, по подходящ ненатрапчив, но и информативен начин

\section{Тестване на реактивните системи}

Реактивните системи се базират на добре познатите ни софтуерни принципи, като слаба свързаност, единична отговорност на компонентите, и други. Това отваря много възможности за тяхното тестване, както модулно, така и интеграционно или системно, които са допълнително улеснени от минималното споделено състояние.

Функционалните абстракции са най-лесно тестваеми, базирани на подаден вход и очакван изход. Абстракциите, като асинхронните \englishterm{future} обекти, които могат да съдържат странични ефекти, могат да бъдат заменени от напълно детерминистична тяхна версия (като \code{Future.successful}). При интеграционни тестове на коректността заедно със странични ефекти може да се използват блокиращите примитиви, които представихме в \shortlabeledref{секция}{sec:concurrent-future-and-promise}. Това подпомага тяхното гранулярно използване, каквото имаме при тестването.

При Akka всеки актьор реално се реализира чрез синхронни ООП обекти, което позволява да бъде тестван техния клас без да е необходима асинхронност.

Най-много грешки се проявяват обаче при комуникация и затова в Akka и другите библиотеки се предоставят инструменти за лесно интеграционно тестване. Това включва и възможности за тестване върху няколко виртуални машини на Java, тъй като голяма част от комуникацията е между възли. За системите е характерно множество ограничени времева за отговор, които е добре да бъдат изключени по време на тестване. Повечето библиотеки разширяват познатите ни модулни тестове за различни сценарии.

\section{Внедряване}

Общата архитектура, представена на \shortlabeledref{фигура}{fig:reactive-architecture} съвпада и с физическото ѝ внедряване. Всеки елемент от нея се намира на отделна машина, като всеки от възлите комуникира с другите по локалната мрежа.

Изграденият клъстер от актьорски системи е устойчив на деления в мрежата, но за оптималната си работа разчита на това, че те ще са по-редки и ще бъдат отстранявани сравнително бързо. Затова възлите в него обикновено са локализирани близо един до друг, с цел ограничаване на тези проблеми.

Благодарение на съобщенията и на силно ограниченото съществено състояние в системата, компонентите са слабо свързани помежду и лесно могат да бъдат добавяни или премахвани, без това да се отразява на работата на системата. Това прави внедряването на нови компоненти или обновяване на софтуера изключително лесно. За стартиране на нова инстанция се изисква единствено стартиране на предварително подготвен пакет с обща конфигурация. Така реактивните системи се вписват удобно в различни облачни услуги, като \englishterm{Heroku}, \englishterm{Amazon Web Services} и много други, които предоставят именно това, позволявайки автоматична еластичност чрез програмни интерфейси или мониторинг. Обновяването на софтуера на реактивните системи при тези облачни услуги често е свързано единствено с постепенното паралелно стартиране на нови и спиране на старите възли, съответно с по-новата/по-старата версия на софтуера.